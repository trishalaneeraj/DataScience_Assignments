{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "labels = model.predict(admissions[[\"gpa\"]])\n",
    "admissions[\"predicted_label\"] = labels\n",
    "\n",
    "#Use the Series method value_counts and the print function to display the distribution of the values in the label column.\n",
    "print(admissions[\"predicted_label\"].value_counts())\n",
    "print(admissions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The admissions Dataframe now contains the predicted value for that row, in the predicted_label column, and the actual value for that row, in the admit column. This format makes it easier for us to calculate how effective the model was on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = model.predict(admissions[[\"gpa\"]])\n",
    "admissions[\"predicted_label\"] = labels\n",
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "\n",
    "#Compare the predicted_label column with the actual_label column.Use a double equals sign (==) to compare the 2 Series objects and assign the resulting Series object to matches.\n",
    "matches = admissions[\"actual_label\"] == admissions[\"predicted_label\"]\n",
    "\n",
    "#Use conditional filtering to filter admissions to just the rows where matches is True. Assign the resulting Dataframe to correct_predictions. \n",
    "\n",
    "correct_predictions = admissions[matches]\n",
    "#matches will take only the true value\n",
    "\n",
    "print(correct_predictions.head())\n",
    "accuracy = len(correct_predictions) / len(admissions)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "\n",
    "true_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 0)\n",
    "true_negatives = len(admissions[true_negative_filter])\n",
    "\n",
    "print(true_positives)\n",
    "print(true_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the previous screen\n",
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "false_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 1)\n",
    "false_negatives = len(admissions[false_negative_filter])\n",
    "\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From previous screens\n",
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "false_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 1)\n",
    "false_negatives = len(admissions[false_negative_filter])\n",
    "true_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 0)\n",
    "true_negatives = len(admissions[true_negative_filter])\n",
    "false_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 0)\n",
    "false_positives = len(admissions[false_positive_filter])\n",
    "\n",
    "specificity = (true_negatives) / (false_positives + true_negatives)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the specificity of the model is 96.25%. This means that the model is really good at knowing which applicants to reject. Since around only 7% of applicants were accepted that applied, it's important that the model reject people correctly who wouldn't have otherwise been acepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
