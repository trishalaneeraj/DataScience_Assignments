{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width          species\n",
      "85            6.0          3.4           4.5          1.6  Iris-versicolor\n",
      "62            6.0          2.2           4.0          1.0  Iris-versicolor\n",
      "43            5.0          3.5           1.6          0.6      Iris-setosa\n",
      "25            5.0          3.0           1.6          0.2      Iris-setosa\n",
      "140           6.7          3.1           5.6          2.4   Iris-virginica\n",
      "['Iris-versicolor' 'Iris-setosa' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read in dataset\n",
    "iris = pandas.read_csv(\"iris.data.csv\")\n",
    "\n",
    "# shuffle rows\n",
    "shuffled_rows = np.random.permutation(iris.index)\n",
    "iris = iris.loc[shuffled_rows,:]\n",
    "\n",
    "print(iris.head())\n",
    "\n",
    "# There are 2 species\n",
    "print(iris.species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17]]\n"
     ]
    }
   ],
   "source": [
    "z = np.asarray([[9, 5, 4]])\n",
    "y = np.asarray([[-1, 2, 4]])\n",
    "\n",
    "# np.dot is used for matrix multiplication\n",
    "# z is 1x3 and y is 1x3,  z * y.T is then 3x3\n",
    "print(np.dot(z,y.T))\n",
    "\n",
    "# Variables to test sigmoid_activation\n",
    "iris[\"ones\"] = np.ones(iris.shape[0])\n",
    "X = iris[['ones', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y = (iris.species == 'Iris-versicolor').values.astype(int)\n",
    "\n",
    "# The first observation\n",
    "x0 = X[0]\n",
    "\n",
    "# Initialize thetas randomly \n",
    "theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "def sigmoid_activation(x, theta):\n",
    "    x = np.asarray(x)\n",
    "    theta = np.asarray(theta)\n",
    "    return 1 / (1 + np.exp(-np.dot(theta.T, x)))\n",
    "                \n",
    "a1 = sigmoid_activation(x0, theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First observation's features and target\n",
    "x0 = X[0]\n",
    "y0 = y[0]\n",
    "\n",
    "# Initialize parameters, we have 5 units and just 1 layer\n",
    "theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "def singlecost(X, y, theta):\n",
    "    # Compute activation\n",
    "    h = sigmoid_activation(X.T, theta)\n",
    "    # Take the negative average of target*log(activation) + (1-target) * log(1-activation)\n",
    "    cost = -np.mean(y * np.log(h) + (1-y) * np.log(1-h))\n",
    "    return cost\n",
    "\n",
    "first_cost = singlecost(x0, y0, theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "\n",
    "# Store the updates into this array\n",
    "grads = np.zeros(theta_init.shape)\n",
    "\n",
    "# Number of observations \n",
    "n = X.shape[0]\n",
    "for j, obs in enumerate(X):\n",
    "    # Compute activation\n",
    "    h = sigmoid_activation(obs, theta_init)\n",
    "    # Get delta\n",
    "    delta = (y[j]-h) * h * (1-h) * obs\n",
    "    # accumulate\n",
    "    grads += delta[:,np.newaxis]/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_init = np.random.normal(0,0.01,size=(5,1))\n",
    "\n",
    "# set a learning rate\n",
    "learning_rate = 0.1\n",
    "# maximum number of iterations for gradient descent\n",
    "maxepochs = 10000       \n",
    "# costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.0001  \n",
    "\n",
    "def learn(X, y, theta, learning_rate, maxepochs, convergence_thres):\n",
    "    costs = []\n",
    "    cost = singlecost(X, y, theta)  # compute initial cost\n",
    "    costprev = cost + convergence_thres + 0.01  # set an inital costprev to past while loop\n",
    "    counter = 0  # add a counter\n",
    "    # Loop through until convergence\n",
    "    for counter in range(maxepochs):\n",
    "        grads = np.zeros(theta.shape)\n",
    "        for j, obs in enumerate(X):\n",
    "            h = sigmoid_activation(obs, theta)   # Compute activation\n",
    "            delta = (y[j]-h) * h * (1-h) * obs   # Get delta\n",
    "            grads += delta[:,np.newaxis]/X.shape[0]  # accumulate\n",
    "        \n",
    "        # update parameters \n",
    "        theta += grads * learning_rate\n",
    "        counter += 1  # count\n",
    "        costprev = cost  # store prev cost\n",
    "        cost = singlecost(X, y, theta) # compute new cost\n",
    "        costs.append(cost)\n",
    "        if np.abs(costprev-cost) < convergence_thres:\n",
    "            break\n",
    "        \n",
    "    plt.plot(costs)\n",
    "    plt.title(\"Convergence of the Cost Function\")\n",
    "    plt.ylabel(\"J($\\Theta$)\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    return theta\n",
    "        \n",
    "theta = learn(X, y, theta_init, learning_rate, maxepochs, convergence_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta0_init = np.random.normal(0,0.01,size=(5,4))\n",
    "theta1_init = np.random.normal(0,0.01,size=(5,1))\n",
    "def feedforward(X, theta0, theta1):\n",
    "    # feedforward to the first layer\n",
    "    a1 = sigmoid_activation(X.T, theta0).T\n",
    "    # add a column of ones for bias term\n",
    "    a1 = np.column_stack([np.ones(a1.shape[0]), a1])\n",
    "    # activation units are then inputted to the output layer\n",
    "    out = sigmoid_activation(a1.T, theta1)\n",
    "    return out\n",
    "\n",
    "h = feedforward(X, theta0_init, theta1_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta0_init = np.random.normal(0,0.01,size=(5,4))\n",
    "theta1_init = np.random.normal(0,0.01,size=(5,1))\n",
    "\n",
    "# X and y are in memory and should be used as inputs to multiplecost()\n",
    "def multiplecost(X, y, theta0, theta1):\n",
    "    # feed through network\n",
    "    h = feedforward(X, theta0, theta1) \n",
    "    # compute error\n",
    "    inner = y * np.log(h) + (1-y) * np.log(1-h)\n",
    "    # negative of average error\n",
    "    return -np.mean(inner)\n",
    "\n",
    "c = multiplecost(X, y, theta0_init, theta1_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a class for this model, it's good practice and condenses the code\n",
    "class NNet3:\n",
    "    def __init__(self, learning_rate=0.5, maxepochs=1e4, convergence_thres=1e-5, hidden_layer=4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.maxepochs = int(maxepochs)\n",
    "        self.convergence_thres = 1e-5\n",
    "        self.hidden_layer = int(hidden_layer)\n",
    "        \n",
    "    def _multiplecost(self, X, y):\n",
    "        # feed through network\n",
    "        l1, l2 = self._feedforward(X) \n",
    "        # compute error\n",
    "        inner = y * np.log(l2) + (1-y) * np.log(1-l2)\n",
    "        # negative of average error\n",
    "        return -np.mean(inner)\n",
    "    \n",
    "    def _feedforward(self, X):\n",
    "        # feedforward to the first layer\n",
    "        l1 = sigmoid_activation(X.T, self.theta0).T\n",
    "        # add a column of ones for bias term\n",
    "        l1 = np.column_stack([np.ones(l1.shape[0]), l1])\n",
    "        # activation units are then inputted to the output layer\n",
    "        l2 = sigmoid_activation(l1.T, self.theta1)\n",
    "        return l1, l2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, y = self._feedforward(X)\n",
    "        return y\n",
    "    \n",
    "    def learn(self, X, y):\n",
    "        nobs, ncols = X.shape\n",
    "        self.theta0 = np.random.normal(0,0.01,size=(ncols,self.hidden_layer))\n",
    "        self.theta1 = np.random.normal(0,0.01,size=(self.hidden_layer+1,1))\n",
    "        \n",
    "        self.costs = []\n",
    "        cost = self._multiplecost(X, y)\n",
    "        self.costs.append(cost)\n",
    "        costprev = cost + self.convergence_thres+1  # set an inital costprev to past while loop\n",
    "        counter = 0  # intialize a counter\n",
    "\n",
    "        # Loop through until convergence\n",
    "        for counter in range(self.maxepochs):\n",
    "            # feedforward through network\n",
    "            l1, l2 = self._feedforward(X)\n",
    "\n",
    "            # Start Backpropagation\n",
    "            # Compute gradients\n",
    "            l2_delta = (y-l2) * l2 * (1-l2)\n",
    "            l1_delta = l2_delta.T.dot(self.theta1.T) * l1 * (1-l1)\n",
    "\n",
    "            # Update parameters by averaging gradients and multiplying by the learning rate\n",
    "            self.theta1 += l1.T.dot(l2_delta.T) / nobs * self.learning_rate\n",
    "            self.theta0 += X.T.dot(l1_delta)[:,1:] / nobs * self.learning_rate\n",
    "            \n",
    "            # Store costs and check for convergence\n",
    "            counter += 1  # Count\n",
    "            costprev = cost  # Store prev cost\n",
    "            cost = self._multiplecost(X, y)  # get next cost\n",
    "            self.costs.append(cost)\n",
    "            if np.abs(costprev-cost) < self.convergence_thres and counter > 500:\n",
    "                break\n",
    "\n",
    "# Set a learning rate\n",
    "learning_rate = 0.5\n",
    "# Maximum number of iterations for gradient descent\n",
    "maxepochs = 10000       \n",
    "# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.00001  \n",
    "# Number of hidden units\n",
    "hidden_units = 4\n",
    "\n",
    "# Initialize model \n",
    "model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,\n",
    "              convergence_thres=convergence_thres, hidden_layer=hidden_units)\n",
    "# Train model\n",
    "model.learn(X, y)\n",
    "\n",
    "# Plot costs\n",
    "plt.plot(model.costs)\n",
    "plt.title(\"Convergence of the Cost Function\")\n",
    "plt.ylabel(\"J($\\Theta$)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First 70 rows to X_train and y_train\n",
    "# Last 30 rows to X_train and y_train\n",
    "X_train = X[:70]\n",
    "y_train = y[:70]\n",
    "\n",
    "X_test = X[-30:]\n",
    "y_test = y[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Set a learning rate\n",
    "learning_rate = 0.5\n",
    "# Maximum number of iterations for gradient descent\n",
    "maxepochs = 10000       \n",
    "# Costs convergence threshold, ie. (prevcost - cost) > convergence_thres\n",
    "convergence_thres = 0.00001  \n",
    "# Number of hidden units\n",
    "hidden_units = 4\n",
    "\n",
    "# Initialize model \n",
    "model = NNet3(learning_rate=learning_rate, maxepochs=maxepochs,\n",
    "              convergence_thres=convergence_thres, hidden_layer=hidden_units)\n",
    "model.learn(X_train, y_train)\n",
    "\n",
    "yhat = model.predict(X_test)[0]\n",
    "\n",
    "auc = roc_auc_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
